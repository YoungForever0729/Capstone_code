{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset label counts:\n",
      "Label 5: 5421 samples\n",
      "Label 0: 5923 samples\n",
      "Label 4: 5842 samples\n",
      "Label 1: 6742 samples\n",
      "Label 9: 5949 samples\n",
      "Label 2: 5958 samples\n",
      "Label 3: 6131 samples\n",
      "Label 6: 5918 samples\n",
      "Label 7: 6265 samples\n",
      "Label 8: 5851 samples\n",
      "Client 0 label counts:\n",
      "[593, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Client 1 label counts:\n",
      "[593, 750, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Client 2 label counts:\n",
      "[593, 750, 745, 0, 0, 0, 0, 0, 0, 0]\n",
      "Client 3 label counts:\n",
      "[593, 750, 745, 876, 0, 0, 0, 0, 0, 0]\n",
      "Client 4 label counts:\n",
      "[593, 750, 745, 876, 974, 0, 0, 0, 0, 0]\n",
      "Client 5 label counts:\n",
      "[593, 750, 745, 876, 974, 1085, 0, 0, 0, 0]\n",
      "Client 6 label counts:\n",
      "[593, 750, 745, 876, 974, 1085, 1480, 0, 0, 0]\n",
      "Client 7 label counts:\n",
      "[593, 750, 745, 876, 974, 1085, 1480, 2089, 0, 0]\n",
      "Client 8 label counts:\n",
      "[593, 750, 745, 876, 974, 1085, 1480, 2089, 2926, 0]\n",
      "Client 9 label counts:\n",
      "[586, 742, 743, 875, 972, 1081, 1478, 2087, 2925, 5949]\n",
      "Round 1 Test Loss: 1.2427 and Test Accuracy: 66.58 %\n",
      "Round 2 Test Loss: 0.9051 and Test Accuracy: 73.81 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Subset\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "# 设置随机种子\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "# 设置一个固定的随机种子，例如 42\n",
    "set_seed(42)\n",
    "\n",
    "# 定义一个简单的神经网络\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 200)\n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 创建模型\n",
    "def create_model():\n",
    "    model = SimpleNN()\n",
    "    return model\n",
    "\n",
    "# 根据标签将训练集分配到不同客户端\n",
    "def distribute_data_to_clients(train_dataset, num_clients=10):\n",
    "    # 统计数据集原本的每个标签的数量\n",
    "    original_label_counts = Counter(train_dataset.targets.tolist())\n",
    "    print(\"Original dataset label counts:\")\n",
    "    for label, count in original_label_counts.items():\n",
    "        print(f\"Label {label}: {count} samples\")\n",
    "\n",
    "    clients = [[] for _ in range(num_clients)]\n",
    "\n",
    "    for label in range(num_clients):\n",
    "        indices = [i for i, target in enumerate(train_dataset.targets) if target == label]\n",
    "        num_splits = num_clients - label\n",
    "        split_indices = torch.chunk(torch.tensor(indices), num_splits)\n",
    "        for i in range(num_splits):\n",
    "            clients[label + i].extend(split_indices[i].tolist())\n",
    "\n",
    "    client_subsets = [Subset(train_dataset, client_data) for client_data in clients]\n",
    "\n",
    "    # 输出每个客户端的每个标签的数量\n",
    "    for i, client_data in enumerate(client_subsets):\n",
    "        client_targets = [train_dataset.targets[idx] for idx in client_data.indices]\n",
    "        client_label_counts = Counter(client_targets)\n",
    "\n",
    "        print(f\"Client {i} label counts:\")\n",
    "        l = [0,0,0,0,0,0,0,0,0,0]\n",
    "        for label, count in client_label_counts.items():\n",
    "            if label == 0:\n",
    "                l[0] += 1\n",
    "            if label == 1:\n",
    "                l[1] += 1\n",
    "            if label == 2:\n",
    "                l[2] += 1\n",
    "            if label == 3:\n",
    "                l[3] += 1\n",
    "            if label == 4:\n",
    "                l[4] += 1\n",
    "            if label == 5:\n",
    "                l[5] += 1\n",
    "            if label == 6:\n",
    "                l[6] += 1\n",
    "            if label == 7:\n",
    "                l[7] += 1\n",
    "            if label == 8:\n",
    "                l[8] += 1\n",
    "            if label == 9:\n",
    "                l[9] += 1\n",
    "        print(l)\n",
    "\n",
    "    return client_subsets\n",
    "\n",
    "class EWC(object):\n",
    "    def __init__(self, model, dataloader):\n",
    "        self.model = model\n",
    "        self.dataloader = dataloader\n",
    "\n",
    "        self.params = {n: p for n, p in self.model.named_parameters() if p.requires_grad}\n",
    "        self._means = {}\n",
    "        self._precision_matrices = self._diag_fisher()\n",
    "\n",
    "        # Store a copy of model's current parameters as the mean (theta*)\n",
    "        for n, p in deepcopy(self.params).items():\n",
    "            self._means[n] = p.data.clone()\n",
    "\n",
    "    def _diag_fisher(self):\n",
    "        precision_matrices = {}\n",
    "        for n, p in deepcopy(self.params).items():\n",
    "            p.data.zero_()\n",
    "            precision_matrices[n] = p.data.clone()\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        # Use DataLoader for iterating over batches\n",
    "        for inputs, targets in self.dataloader:\n",
    "            self.model.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            output = self.model(inputs)\n",
    "            loss = nn.CrossEntropyLoss()(output, targets)\n",
    "            # loss = F.nll_loss(F.log_softmax(output, dim=1), targets)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Update precision matrices\n",
    "            for n, p in self.model.named_parameters():\n",
    "                precision_matrices[n] += p.grad.data ** 2 / len(self.dataloader.dataset)\n",
    "\n",
    "        precision_matrices = {n: p for n, p in precision_matrices.items()}\n",
    "        return precision_matrices\n",
    "\n",
    "    def penalty(self, model):\n",
    "        loss = 0\n",
    "        for n, p in model.named_parameters():\n",
    "            # Penalty term based on precision and distance from original weights\n",
    "            _loss = self._precision_matrices[n] * (p - self._means[n]) ** 2\n",
    "            loss += _loss.sum()\n",
    "        return loss\n",
    "\n",
    "# Training loop with EWC regularization\n",
    "def train_with_ewc(model, data_loader, criterion, optimizer, ewc, epochs, importance):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for inputs, targets in data_loader:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(inputs)\n",
    "            loss = criterion(output, targets) + importance * ewc.penalty(model)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # Backward pass and update weights\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "# 联邦加权平均函数，考虑各客户端的数据量\n",
    "def federated_weighted_avg(weights, num_samples):\n",
    "    avg_weights = []\n",
    "    total_samples = sum(num_samples)\n",
    "    for i in range(len(weights[0])):\n",
    "        weighted_sum = sum(weights[j][i] * num_samples[j] / total_samples for j in range(len(weights)))\n",
    "        avg_weights.append(weighted_sum)\n",
    "    return avg_weights\n",
    "\n",
    "def federated_learning(global_model, client_subsets, criterion, hyperparams, test_loader=None):\n",
    "    num_rounds = hyperparams.get('num_rounds', 5)\n",
    "    learning_rate = hyperparams.get('learning_rate', 0.01)\n",
    "    batch_size = hyperparams.get('batch_size', 64)\n",
    "    epochs_per_client = hyperparams.get('epochs_per_client', 1)\n",
    "    lambda_ewc = hyperparams.get('lambda_ewc', 0.1)\n",
    "\n",
    "    loss_history = []\n",
    "    accuracy_history = []\n",
    "\n",
    "    for round_num in range(num_rounds):\n",
    "        client_weights = []\n",
    "        num_samples = []\n",
    "\n",
    "        for client_data in client_subsets:\n",
    "            client_loader = torch.utils.data.DataLoader(client_data, batch_size=batch_size, shuffle=True)\n",
    "            model = create_model()\n",
    "            model.load_state_dict(global_model.state_dict())\n",
    "            \n",
    "            ewc = EWC(model, client_loader)\n",
    "            optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "            train_with_ewc(model, client_loader, criterion, optimizer, ewc, epochs=epochs_per_client, importance=lambda_ewc)\n",
    "\n",
    "            model_weights = [param.data.clone() for param in model.parameters()]\n",
    "            client_weights.append(model_weights)\n",
    "            num_samples.append(len(client_data))\n",
    "\n",
    "        new_weights = federated_weighted_avg(client_weights, num_samples)\n",
    "        state_dict = global_model.state_dict()\n",
    "        new_state_dict = {key: value for key, value in zip(state_dict.keys(), new_weights)}\n",
    "        global_model.load_state_dict(new_state_dict)\n",
    "\n",
    "        if test_loader is not None:\n",
    "            test_loss, accuracy = test(global_model, test_loader, criterion)\n",
    "            loss_history.append(test_loss)\n",
    "            accuracy_history.append(accuracy)\n",
    "            print(f'Round {round_num + 1} Test Loss: {test_loss:.4f} and Test Accuracy: {accuracy * 100:.2f} %')\n",
    "\n",
    "    if test_loader is not None and loss_history and accuracy_history:\n",
    "        plot_loss_accuracy_history(loss_history, accuracy_history)\n",
    "\n",
    "    final_accuracy = accuracy_history[-1] if accuracy_history else None\n",
    "\n",
    "    return loss_history, accuracy_history, final_accuracy\n",
    "\n",
    "# 测试函数\n",
    "def test(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, targets in data_loader:\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss += loss.item() * data.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "    accuracy = correct / total\n",
    "    test_loss /= total\n",
    "    return test_loss, accuracy\n",
    "\n",
    "def plot_loss_accuracy_history(loss_history, accuracy_history):\n",
    "    \"\"\"\n",
    "    绘制测试损失和准确率随联邦学习轮次的变化图。\n",
    "\n",
    "    参数:\n",
    "    - loss_history: 损失历史列表。\n",
    "    - accuracy_history: 准确率历史列表。\n",
    "    \"\"\"\n",
    "    x_values = list(range(1, len(loss_history) + 1))\n",
    "    y_values_loss = loss_history\n",
    "    y_values_accuracy = [acc * 100 for acc in accuracy_history]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x_values, y_values_loss)\n",
    "    plt.xlabel('Federated Learning Rounds')\n",
    "    plt.ylabel('Test Loss')\n",
    "    plt.title('Test Loss vs Federated Learning Rounds')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x_values, y_values_accuracy)\n",
    "    plt.xlabel('Federated Learning Rounds')\n",
    "    plt.ylabel('Test Accuracy (%)')\n",
    "    plt.title('Test Accuracy vs Federated Learning Rounds')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    final_accuracy = accuracy_history[-1]\n",
    "    print(f'Final Test Accuracy: {final_accuracy * 100:.2f} %')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 加载 MNIST 数据集\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# 调用函数\n",
    "client_subsets = distribute_data_to_clients(train_dataset, num_clients=10)\n",
    "\n",
    "# 创建测试数据加载器\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# 调用 federated_learning 函数时传入超参数字典\n",
    "hyperparams = {\n",
    "    'num_rounds': 10,\n",
    "    'learning_rate': 0.01,\n",
    "    'batch_size': 64,\n",
    "    'epochs_per_client': 5,\n",
    "    'lambda_ewc': 0.01\n",
    "}\n",
    "\n",
    "loss_history, accuracy_history, final_accuracy = federated_learning(\n",
    "    global_model=create_model(),\n",
    "    client_subsets=client_subsets,\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    hyperparams=hyperparams,\n",
    "    test_loader=test_loader\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
